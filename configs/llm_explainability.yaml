llm_explainability:
  enabled: true
  
  lm_studio:
    base_url: "http://localhost:1234/v1"
    model: "openai/gpt-oss-20b"
    temperature: 0.0
    max_tokens: 512
    timeout: 10
  
  caching:
    enabled: true
    max_cache_size: 1000
  
  fallback:
    use_rule_based: true
    continue_on_failure: true
  
  output:
    save_to_file: true
    output_dir: "results/explanations"
    format: "json"
  
  validation:
    strict_schema: true
    required_fields:
      - summary
      - key_reasons
      - risk_level
      - rollback_condition
      - confidence_in_decision
  
  logging:
    log_llm_calls: false
    log_failures: true
    log_explanations: true